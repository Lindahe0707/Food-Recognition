{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data augmentation\n",
    "\n",
    "#At every new epoch new random transformations will be applied\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "target_size = (300, 300)\n",
    "batch_size = 20\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "#In validation set, we only rescale the data\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# This is a generator that will read pictures found in 'train' and generate batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'pictures/train',  #This is the target directory\n",
    "        target_size=(300, 300),  #All images will be resized to 300x300\n",
    "        batch_size=batch_size, color_mode = \"rgb\", class_mode='categorical')\n",
    "\n",
    "# This is a similar generator for validation data\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'pictures/validation',\n",
    "        target_size=(300, 300), batch_size=batch_size, color_mode = \"rgb\", class_mode='categorical', shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************** train own CNN model**********************\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "train_num = 2585\n",
    "val_num = 699\n",
    "\n",
    "#Method 1 - build a new simple model and train\n",
    "\n",
    "def create_base():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(300, 300, 3))) #input layer\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#The model so far outputs 3D feature maps (height, width, features)\n",
    "\n",
    "    model.add(Flatten())  #Convert 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(64)) #Usually be 64, 128, 256...\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(6))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "# Compile\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_1 = ModelCheckpoint('own_model_best.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
    "\n",
    "model_1 = create_base()\n",
    "\n",
    "model_1.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch= train_num // batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps= val_num // batch_size, callbacks = [checkpoint_1])\n",
    "\n",
    "#model_1.save_weights('model_1_epochs.h5')  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************** Transfer learning on pre-trained model**********************\n",
    "\n",
    "#Method 2 - train on Xception + all layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "# Dimensions from the image after being resized\n",
    "shape = [300, 300, 3]\n",
    "\n",
    "model_xception = tf.keras.applications.xception.Xception(\n",
    "                  include_top=False,\n",
    "                  weights= 'imagenet',\n",
    "                  input_shape=shape,\n",
    "                  pooling='max'\n",
    "                )\n",
    "model_2 = tf.keras.Sequential()\n",
    "model_2.add(model_xception)\n",
    "model_2.add(tf.keras.layers.Flatten())\n",
    "model_2.add(tf.keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "#Compile\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "model_2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_2 = ModelCheckpoint('Xception_all_layers.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
    "\n",
    "model_2.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch= train_num // batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps= val_num // batch_size, callbacks = [checkpoint_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 3 - train on VGG + only on last four layers + a full connected layer\n",
    "\n",
    "model_vgg = VGG16(include_top=False, weights='imagenet',input_shape=(300, 300, 3))\n",
    "print('Model loaded.')\n",
    "\n",
    "# Freeze the layers except the last 4 layers\n",
    "for layer in model_vgg.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model_3 = Sequential()\n",
    "model_3.add(model_vgg)\n",
    "\n",
    "model_3.add(Flatten())\n",
    "model_3.add(Dense(1024, activation='relu'))\n",
    "model_3.add(Dropout(0.5))\n",
    "model_3.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model_3.summary()\n",
    "\n",
    "#Compile\n",
    "model_3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_3 = ModelCheckpoint('VGG_with_freeze.h5', save_weights_only = True, verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  \n",
    "\n",
    "model_3.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch= train_num // batch_size,\n",
    "        epochs= 25,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps= val_num // batch_size, \n",
    "    callbacks = [checkpoint_3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 4 - Train on VGG + all layers (no freeze)\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input, Flatten, Dense, Dropout\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "#Get the convolutional part of a VGG network trained on ImageNet\n",
    "model_vgg6_conv = VGG16(\n",
    "                  include_top=False,\n",
    "                  weights='imagenet',\n",
    "                  input_shape= (300, 300, 3))\n",
    "\n",
    "model_4 = Sequential()\n",
    "model_4.add(model_vgg6_conv)\n",
    "model_4.add(Flatten())\n",
    "model_4.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model_4.summary()\n",
    "\n",
    "#Compile\n",
    "model_4.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_4 = ModelCheckpoint('VGG_all_layers.h5', save_weights_only = True, verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  \n",
    "\n",
    "model_4.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch= train_num // batch_size,\n",
    "        epochs= 30,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps= val_num // batch_size, \n",
    "    callbacks = [checkpoint_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 5 - Train on Inception V3 + all layers\n",
    "\n",
    "#Build the inception v3 network\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "model_inception = InceptionV3(include_top=False, weights='imagenet',input_shape=(300, 300, 3))\n",
    "print('Model loaded.')\n",
    "\n",
    "#model_inception.summary()\n",
    "    \n",
    "model_5 = Sequential()\n",
    "model_5.add(model_inception)\n",
    "model_5.add(Flatten())\n",
    "model_5.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model_5.summary()\n",
    "\n",
    "#Compile\n",
    "model_5.compile(loss='categorical_crossentropy',\n",
    "              optimizer= optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_5 = ModelCheckpoint('Inception_all_layers.h5', verbose=1, monitor='val_categorical_accuracy',save_best_only=True, save_weights_only = True, mode='auto')  \n",
    "\n",
    "model_5.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch= train_num // batch_size,\n",
    "        epochs= 20,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps= val_num // batch_size, callbacks = [checkpoint_5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1: the performance of own model\n",
    "#Accuracy on validation set = 0.422\n",
    "\n",
    "#Model 2: the performance of Xception + full train\n",
    "#Accuracy on validation set = 0.536\n",
    "\n",
    "#Model 3: the performance of VGG 16 + freeze layers\n",
    "#Accuracy on validation set = 0.551\n",
    "\n",
    "#Model 4: the performance of Vgg16 + full train\n",
    "#Accuracy on validation set = 0.476\n",
    "\n",
    "#Model 5: the performance of InceptionV3 + full train\n",
    "#Accuracy on validation set = 0.658"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
